import fs from "fs";
import path from "path";
import glob from "fast-glob";
import readline from "readline";
import { OpenAI } from "openai";
import chalk from "chalk";
import { config } from "dotenv";

config();

// â”€â”€â”€â”€â”€â”€â”€â”€â”€ configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const ROOT = path.resolve(process.argv[2] || ".");
const OUT_DIR = path.join(ROOT, ".shadow");
const MAX_FILE_BYTES = 80_000;        // skip >80 KB blobs entirely
const MAX_SNIPPET_CHARS = 800;        // perâ€‘file snippet budget
const MAX_GLOBAL_CHARS = 60_000;      // repoâ€‘wide snippet budget
const OVERVIEW_TOKEN_LIMIT = 150;     // token budget for overview
const SECTION_TOKEN_LIMIT = 150;      // token budget for each section
const MODEL = process.env.MODEL || "gpt-4o-mini";

// ignore rules (similar to .gitignore)
const IGNORE_GLOBS = [
  "**/node_modules/**",
  "**/.git/**",
  "**/dist/**",
  "**/build/**",
  "**/*.png","**/*.jpg","**/*.jpeg","**/*.gif","**/*.svg","**/*.ico",
  "**/*.lock","**/*.min.*","**/*.map","**/*.woff*","**/*.eot",
  "**/*.class","**/*.exe",
  "**/.shadow/**",
];

// â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const bold = (s: string) => chalk.bold.cyan(s);

function readSnippets(absPath: string): string {
  const buf = fs.readFileSync(absPath);
  if (buf.length === 0 || buf.length > MAX_FILE_BYTES) return "";

  const content = buf.toString("utf8");
  const lines = content.split(/\r?\n/);
  const start = lines.slice(0, 15).join("\n");
  const midIndex = Math.floor(lines.length / 2);
  const mid = lines.slice(Math.max(0, midIndex - 7), midIndex + 8).join("\n");

  let snippet = start;
  if (mid && mid !== start) snippet += "\n...\n" + mid;
  return snippet.slice(0, MAX_SNIPPET_CHARS);
}

async function promptYesNo(question: string) {
  const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
  return new Promise<boolean>((resolve) => {
    rl.question(question + " (y/N) ", (ans) => {
      rl.close();
      resolve(/^y(es)?$/i.test(ans.trim()));
    });
  });
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€ openai wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const openai = new OpenAI();

async function chat(
  messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],
  expectJson = false,
) {
  const res = await openai.chat.completions.create({
    model: MODEL,
    temperature: 0.2,
    messages,
    ...(expectJson && { response_format: { type: "json_object" as const } }),
  });
  return res.choices[0]?.message?.content?.trim() || "";
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€ main generation logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
(async () => {
  if (!process.env.OPENAI_API_KEY) {
    console.error("OPENAI_API_KEY env var not set");
    process.exit(1);
  }
  console.log(bold(`ðŸ“š DeepWikiâ€‘LLM: scanning ${ROOT} â€¦`));

  // Gather file list
  const entries = await glob("**/*", {
    cwd: ROOT,
    absolute: true,
    dot: true,
    ignore: IGNORE_GLOBS,
  });

  // Read snippets
  const fileMeta: { rel: string; snippet: string }[] = [];
  for (const abs of entries) {
    if (!fs.statSync(abs).isFile()) continue;
    const rel = path.relative(ROOT, abs);
    const snippet = readSnippets(abs);
    fileMeta.push({ rel, snippet });
  }

  // Manifest shown to the LLM
  let manifest = "# Repository Manifest\n\n";
  manifest += "| File | Size (bytes) |\n|------|--------------|\n" +
    fileMeta.map((f) => {
      const size = fs.statSync(path.join(ROOT, f.rel)).size;
      return `| ${f.rel} | ${size} |`;
    }).join("\n") + "\n";

  // Concatenate snippets with global cap
  let allSnips = "";
  for (const f of fileMeta) {
    allSnips += `\n// ===== ${f.rel} =====\n${f.snippet}\n`;
    if (allSnips.length > MAX_GLOBAL_CHARS) break;
  }

  // â”€â”€â”€â”€â”€ 1ï¸âƒ£ Decide wiki structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  console.log(bold("ðŸ§   Step 1: generating TOC & overview â€¦"));
  const tocPrompt: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    {
      role: "system",
      content:
        `You are DeepWikiâ€‘LLM, master of crystalâ€‘clear DeepWiki documentation.\n` +
        `Output MUST be valid JSON with keys: overview_md (string) and sections (array {id,title,file_globs}).\n` +
        `â€¢ overview_md: â‰¤ ${OVERVIEW_TOKEN_LIMIT} tokens, use bullet lists, embed wikiâ€‘links ([[ ]]) to each section id.\n` +
        `â€¢ sections: choose precise, conceptually clean titles, no \"Misc\".\n` +
        `â€¢ file_globs: glob patterns or commaâ€‘separated rel paths most relevant to section.\n`,
    },
    {
      role: "user",
      content:
        manifest +
        "\n\n## CODE SNIPPETS (truncated):\n```txt\n" +
        allSnips +
        "\n```",
    },
  ];

  const tocJsonRaw = await chat(tocPrompt, true);
  let toc: {
    overview_md: string;
    sections: { id: string; title: string; file_globs: string }[];
  };

  try {
    toc = JSON.parse(tocJsonRaw);
  } catch {
    console.error("âŒ Failed to parse LLM JSON:\n", tocJsonRaw);
    process.exit(1);
  }

  console.log(bold("ðŸ“‘  Sections:"));
  toc.sections.forEach((s, i) =>
    console.log(`  ${i + 1}. ${s.title}  [${s.file_globs}]`),
  );

  if (!(await promptYesNo("Proceed with these sections?"))) process.exit(0);

  fs.mkdirSync(OUT_DIR, { recursive: true });
  const nowIso = new Date().toISOString();

  // â”€â”€â”€â”€â”€ Write overview page â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const overviewFrontMatter =
    `---\n` +
    `id: overview\n` +
    `title: Overview\n` +
    `generated: ${nowIso}\n` +
    `model: ${MODEL}\n` +
    `---\n\n`;

  const overviewLinks =
    "## Sections\n" + toc.sections.map((s) => `- [[${s.id}]] ${s.title}`).join("\n") + "\n";

  fs.writeFileSync(
    path.join(OUT_DIR, "00_OVERVIEW.md"),
    overviewFrontMatter + toc.overview_md.trim() + "\n\n" + overviewLinks,
  );
  console.log(bold("âœ…  00_OVERVIEW.md written"));

  // â”€â”€â”€â”€â”€ 2ï¸âƒ£ Generate each section page â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  for (const sec of toc.sections) {
    const matchedFiles = await glob(
      sec.file_globs.split(/[;,]/).map((g) => g.trim()),
      { cwd: ROOT, absolute: true },
    );

    let context = "";
    matchedFiles.forEach((mf) => {
      const rel = path.relative(ROOT, mf);
      const snip = fileMeta.find((m) => m.rel === rel)?.snippet || readSnippets(mf);
      context += `\n// >>> ${rel}\n${snip}\n`;
    });
    if (!context.trim()) context = "\n(No direct snippets matched; rely on manifest.)";

    console.log(bold(`ðŸ§   Generating Â«${sec.title}Â» â€¦`));
    const secPrompt: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
      {
        role: "system",
        content:
          `You are DeepWikiâ€‘LLM. Produce ONE Markdown doc body (no frontâ€‘matter).\n` +
          `Follow DeepWiki style: terse bullet lists, internal links with [[ ]], no fluff.\n` +
          `Structure:\n` +
          `# ${sec.title}\n` +
          `## Purpose  (â‰¤2 bullets)\n` +
          `## Details  (â‰¤4 bullets)\n` +
          `## Key Files  (wikiâ€‘link each rel path)\n` +
          `## Links  (omit if none)\n` +
          `Length â‰¤ ${SECTION_TOKEN_LIMIT} tokens.\n`,
      },
      {
        role: "user",
        content:
          manifest +
          "\n\n### SNIPPETS SELECTED\n```txt\n" +
          context.slice(0, MAX_GLOBAL_CHARS) +
          "\n```\n",
      },
    ];

    const bodyMd = await chat(secPrompt);
    const frontMatter =
      `---\n` +
      `id: ${sec.id}\n` +
      `title: ${sec.title}\n` +
      `generated: ${nowIso}\n` +
      `model: ${MODEL}\n` +
      `---\n\n`;

    const fname = `${sec.id.replace(/[^a-z0-9\-]+/gi, "_").replace(/^_+|_+$/g, "").toLowerCase()}.md`;
    fs.writeFileSync(path.join(OUT_DIR, fname), frontMatter + bodyMd.trim() + "\n");
    console.log(bold(`âœ…  ${fname} written`));
  }

  console.log(bold("\nðŸŽ‰  DeepWikiâ€‘LLM docs generated at:"), OUT_DIR);
})().catch((err) => {
  console.error(err);
  process.exit(1);
});
