# ğŸ” Shadow Backend: Production Multi-User Analysis & Fix Plan

## **ğŸ“Š SYSTEM ARCHITECTURE OVERVIEW**

### **ğŸ—ï¸ Three-Tier Architecture**

**1. Frontend (Next.js + React)**
- **Location**: `apps/frontend/`
- **WebSocket Client**: Socket.IO with task-specific rooms
- **State Management**: React Query with task-scoped keys
- **Real-time Updates**: Stream chunks, terminal output, file changes
- **Status**: âœ… **Already Task-Scoped** - No issues here

**2. Backend Server (Node.js + Express)**
- **Location**: `apps/server/`
- **Main Components**: HTTP API + WebSocket server
- **Stream Management**: Global state causing data corruption
- **Status**: âŒ **Critical Global State Issues**

**3. Sidecar Container (Express + Socket.IO Client)**
- **Location**: `apps/sidecar/`
- **Runtime**: Kata QEMU VM per task
- **Purpose**: Isolated file/command execution
- **Status**: âœ… **Already Task-Scoped** - No issues here

---

## **ğŸ”„ COMPLETE DATA FLOW ANALYSIS**

### **User Interaction Flow**
```
Frontend â†’ Backend API â†’ Task Initialization â†’ VM Creation â†’ Sidecar Startup
    â†“         â†“              â†“                    â†“            â†“
WebSocket â†’ ChatService â†’ LLM Integration â†’ Tool Execution â†’ File Operations
    â†“         â†“              â†“                    â†“            â†“
Stream     Message        Tool Calls        Remote API    Filesystem
Chunks  â†’ Persistence  â†’ Real-time      â†’ HTTP Calls  â†’ Events
    â†“         â†“              â†“                    â†“            â†“
Frontend   Database     WebSocket        Sidecar       WebSocket
Updates    Storage      Broadcasting     Response      Events
```

### **Task Lifecycle**
1. **Initialization** (`TaskInitializationEngine`)
   - Local: Clone repo to filesystem
   - Remote: Create Kata QEMU VM + clone repo inside VM
   - Status tracking via WebSocket events

2. **Execution** (`ChatService` + `ToolExecutor`)
   - LLM streaming with tool calls
   - File operations via abstraction layer
   - Real-time terminal output

3. **Cleanup** (`TaskCleanupService`)
   - Archive completed tasks
   - Destroy VMs and clean filesystem

---

## **ğŸ”¥ CRITICAL ISSUES IDENTIFIED**

### **ğŸ’¥ Issue #1: Global Stream State (SHOWSTOPPER)**

**Problem Location**: `apps/server/src/socket.ts:33-34`
```typescript
let currentStreamChunks: StreamChunk[] = [];
let isStreaming = false;
```

**Impact**:
- âŒ User A's stream corrupts User B's stream recovery
- âŒ Multiple users see each other's partial responses
- âŒ Stream state recovery fails for concurrent tasks
- âŒ Data leakage between tasks

**Affected Components**:
- `startStream()` - Resets global state
- `endStream()` - Modifies global state  
- `emitStreamChunk()` - Appends to global array
- Stream state recovery in frontend
- WebSocket reconnection handling

### **ğŸ’¥ Issue #2: Single ChatService Instance (MANAGEABLE)**

**Problem Location**: `apps/server/src/app.ts:20`
```typescript
export const chatService = new ChatService();
```

**Impact**:
- âœ… Uses task-keyed Maps internally - actually OK
- âœ… No data corruption between tasks
- ğŸ’¡ Could be optimized with better resource management

### **ğŸ’¥ Issue #3: Terminal ID Counter (MINOR)**

**Problem Location**: `apps/server/src/agent/tools/index.ts:27`
```typescript
let terminalEntryId = 1;
```

**Impact**:
- âš ï¸ Terminal entry IDs not unique across tasks
- âš ï¸ Could cause frontend confusion (non-critical)

---

## **ğŸ—ï¸ COMPLETE COMPONENT ANALYSIS**

### **âœ… COMPONENTS THAT WORK CORRECTLY**

**1. Database Layer (PostgreSQL + Prisma)**
- Task isolation via foreign keys
- Concurrent access handled properly
- Sequence numbers generated correctly per task

**2. WebSocket Room System**
- Clients join `task-${taskId}` rooms
- Events properly scoped to task rooms
- No cross-task contamination

**3. Execution Abstraction Layer**
- Factory pattern creates task-specific executors
- Local/Remote mode abstraction works correctly
- Tool operations properly isolated

**4. Frontend State Management**
- React Query with task-specific keys
- Component state properly scoped
- Stream reconstruction logic correct

**5. Sidecar Architecture**
- Each VM gets its own sidecar instance
- HTTP APIs are stateless and safe
- Filesystem events properly attributed to taskId

**6. Authentication & Authorization**
- GitHub OAuth scoped to users
- API keys managed per user
- No cross-user data leakage

### **âŒ COMPONENTS WITH ISSUES**

**1. Stream State Management**
- Global variables shared across all tasks
- No task-based isolation
- Causes data corruption in multi-user scenarios

**2. Terminal Buffer Management**  
- Global counter for entry IDs
- Minor UX issues with non-unique IDs

---

## **ğŸš€ PRODUCTION READINESS ASSESSMENT**

### **Current State: Single User âœ… | Multi-User âŒ**

**What Works in Production**:
- âœ… Individual task isolation (workspace, database, auth)
- âœ… VM-level security isolation (Kata QEMU)
- âœ… Horizontal database scaling
- âœ… File operation safety
- âœ… Resource cleanup and lifecycle management

**What Breaks with Multiple Users**:
- âŒ Stream state corruption between concurrent tasks
- âŒ Incorrect stream recovery on reconnection  
- âŒ Partial responses shown to wrong users
- âŒ Data leakage in WebSocket events

**Severity**: **CRITICAL** - Will cause immediate data corruption and user confusion

---

## **ğŸ”§ PRODUCTION MULTI-USER FIX PLAN**

### **Phase 1: Fix Critical Stream State Issues** âš¡ (REQUIRED)

#### **Problem Summary**
The server has global stream state variables that cause data corruption when multiple users stream simultaneously. This is a **critical production blocker**.

#### **Core Fix: Task-Scoped Stream State**

**Current (Broken)**:
```typescript
// In socket.ts - SHARED ACROSS ALL TASKS!
let currentStreamChunks: StreamChunk[] = [];
let isStreaming = false;
```

**Solution: Task-Keyed Stream State**:
```typescript
// Task-specific stream state
const taskStreamStates = new Map<string, {
  chunks: StreamChunk[];
  isStreaming: boolean;
}>();
```

#### **Files to Modify**:

1. **`apps/server/src/socket.ts`**
   - Replace global `currentStreamChunks` and `isStreaming` with `Map<taskId, StreamState>`
   - Update `startStream(taskId)`, `endStream(taskId)`, `emitStreamChunk(chunk, taskId)`
   - Fix stream state recovery to be task-specific
   - Update `onStreamState()` handler to use task-specific state

2. **`apps/server/src/agent/tools/index.ts`**
   - Replace global `terminalEntryId` with task-specific counter or UUIDs
   - Update `createAndEmitTerminalEntry()` to use task-scoped IDs

#### **Testing Requirements**:
- âœ… Two users can stream simultaneously without interference  
- âœ… Stream state recovery works correctly per task
- âœ… WebSocket reconnection restores correct task state
- âœ… No data leakage between concurrent tasks

### **Phase 2: Minimal Redis Integration** ğŸš€ (Optional)

#### **ğŸ¯ Goal: Bounded Memory Usage**
Primary objective is preventing memory leaks as task count scales, with horizontal scaling as a bonus benefit.

#### **ğŸ”„ What to Move to Redis (Priority Order)**

**1. Task Stream States** (Highest Impact)
```typescript
// Current: In-memory Map that grows indefinitely
const taskStreamStates = new Map<string, TaskStreamState>();

// Redis: Auto-expiring keys  
// Key: `stream:${taskId}`
// TTL: 1 hour (auto-cleanup of completed tasks)
```

**2. Terminal Polling Intervals** (Medium Impact)
```typescript
// Current: NodeJS.Timeout objects stored in memory
const terminalPollingIntervals = new Map<string, NodeJS.Timeout>();

// Redis: Simple boolean flags
// Key: `terminal:polling:${taskId}` 
// TTL: 2 hours
```

**3. Connection States** (Lower Impact - only if multi-instance)
```typescript
// Current: Socket connection metadata
const connectionStates = new Map<string, ConnectionState>();

// Redis: Only needed for multiple server instances
// Key: `connection:${socketId}`
// TTL: 30 minutes
```

#### **ğŸš« What NOT to Move (Keep Simple)**
- **ChatService Maps**: Already have good cleanup, low memory footprint
- **Database connections**: PostgreSQL handles this well  
- **File system watchers**: Local to each instance, needed for performance

#### **ğŸ”§ Implementation Steps**

**1. Add Redis Client** (5 minutes)
```bash
npm install redis @types/redis
```

**2. Create Redis Service** (15 minutes)
```typescript
// src/services/redis-service.ts
export class RedisService {
  async setTaskStreamState(taskId: string, state: TaskStreamState): Promise<void>
  async getTaskStreamState(taskId: string): Promise<TaskStreamState | null> 
  async deleteTaskStreamState(taskId: string): Promise<void>
  
  async setTerminalPolling(taskId: string, isPolling: boolean): Promise<void>
  async isTerminalPolling(taskId: string): Promise<boolean>
  async deleteTerminalPolling(taskId: string): Promise<void>
}
```

**3. Update Socket.ts** (30 minutes)
```typescript
// Replace Map operations with Redis calls
// Keep same function signatures for compatibility
async function getOrCreateTaskStreamState(taskId: string): Promise<TaskStreamState> {
  return await redisService.getTaskStreamState(taskId) || { chunks: [], isStreaming: false };
}
```

**4. Update Memory Cleanup** (10 minutes)
```typescript  
// Add Redis cleanup to existing MemoryCleanupService
static async cleanupTaskMemory(taskId: string): Promise<void> {
  // ... existing cleanup ...
  await redisService.deleteTaskStreamState(taskId);
  await redisService.deleteTerminalPolling(taskId);
}
```

**5. Add Graceful Fallback** (15 minutes)
```typescript
// Graceful degradation if Redis unavailable
if (!redisService.connected) {
  // Fall back to in-memory Maps (current behavior)
  return memoryFallback.getTaskStreamState(taskId);
}
```

#### **ğŸ“¦ Redis Configuration**

**Key Strategy**:
```
stream:{taskId}                    TTL: 1 hour
terminal:polling:{taskId}          TTL: 2 hours  
connection:{socketId}              TTL: 30 minutes (optional)
```

**Environment Configuration**:
```typescript
// config/shared.ts
export const redis = {
  url: process.env.REDIS_URL || 'redis://localhost:6379',
  enabled: process.env.REDIS_ENABLED === 'true' || process.env.NODE_ENV === 'production'
};
```

**Docker Compose Addition**:
```yaml
redis:
  image: redis:7-alpine
  ports:
    - "6379:6379"
  command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
```

#### **ğŸ“ˆ Benefits**

**Memory Management**:
- âœ… Stream states auto-expire (no memory leaks)
- âœ… Dead task cleanup happens automatically via TTL
- âœ… Memory usage stays bounded even with thousands of tasks

**Horizontal Scaling** (Bonus):
- âœ… Multiple server instances share stream state
- âœ… WebSocket reconnection works across instances  
- âœ… Load balancer can route requests freely

**Operational**:
- âœ… Redis provides built-in monitoring and inspection
- âœ… Can flush all task data externally if needed
- âœ… Development works without Redis (fallback to in-memory)

#### **ğŸ—ï¸ Additional Enhancements** (Future)

**Performance Optimizations**:
- Add per-user concurrent task limits
- Implement task queueing for resource management
- Add connection pooling and rate limiting
- Database query optimizations for high-concurrency scenarios

**Monitoring & Observability**:
- Add task-level metrics and monitoring
- Stream state health checks  
- Connection pool monitoring
- Resource usage tracking per task

---

## **ğŸ“‹ IMPLEMENTATION PRIORITY**

**ğŸ”´ CRITICAL (Phase 1)**: 
- Must be implemented before production deployment
- Estimated effort: 4-6 hours
- **Complexity**: Low-Medium (straightforward refactoring)

**ğŸŸ¡ OPTIONAL (Phase 2)**:
- Can be added incrementally after production deployment
- **Minimal Redis integration**: 2-3 hours
- **Additional enhancements**: +1-2 days each

---

## **ğŸ¯ POST-FIX ARCHITECTURE**

After Phase 1 implementation:
- âœ… **Task Isolation**: Complete task scoping across all components
- âœ… **Multi-User Safe**: Multiple users can work simultaneously 
- âœ… **Production Ready**: Single-instance deployment ready
- âœ… **Horizontally Scalable**: With Phase 2 Redis integration

---

## **âš ï¸ RISK ASSESSMENT**

- **Low Risk**: Changes are isolated to stream state management
- **No Breaking Changes**: Existing task isolation remains intact
- **Backward Compatible**: No API or database changes required
- **Easy Rollback**: Simple revert if issues occur

---

## **ğŸ”„ KEY INTERACTIONS BETWEEN COMPONENTS**

### **Server â†” Frontend Communication**
```
Frontend Socket.IO Client
    â†“ (join task room)
Backend WebSocket Server 
    â†“ (task-scoped events)
Stream State Management (âœ… FIXED)
    â†“ (broadcast to room)
Frontend State Updates
```

### **Server â†” Sidecar Communication**
```
Backend Tool Executor
    â†“ (HTTP API calls)
Sidecar HTTP Server
    â†“ (file operations)
Sidecar Socket.IO Client
    â†“ (filesystem events)
Backend WebSocket Handler
    â†“ (emit to task room)
Frontend Real-time Updates
```

### **Stream Processing Flow**
```
LLM Provider (Anthropic/OpenAI)
    â†“ (streaming tokens)
Stream Processor
    â†“ (structured chunks)
Stream State Manager (âœ… FIXED)
    â†“ (task-scoped storage)
WebSocket Broadcaster
    â†“ (room-specific emit)
Frontend Stream Reconstruction
```

---

## **âœ… CONCLUSION & STATUS**

### **Phase 1: âœ… COMPLETE (2024)**
All critical multi-user concurrency issues have been **successfully resolved**:

- âœ… **Global stream state fixed**: Replaced with task-scoped Maps
- âœ… **Stream function signatures updated**: Added taskId parameters  
- âœ… **Terminal ID counter fixed**: Now task-unique
- âœ… **Memory cleanup service created**: Comprehensive cleanup system
- âœ… **Cleanup integration**: Integrated with existing task lifecycle

### **Current Production Status: READY** ğŸš€
The system is now **fully production-ready** for multiple concurrent users with proper task isolation and memory management.

### **Phase 2: Optional Future Enhancement**
Minimal Redis integration documented above can be implemented when scaling needs require bounded memory usage and horizontal scaling capabilities.